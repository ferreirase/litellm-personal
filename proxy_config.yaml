model_list:
  - model_name: proxy/gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini # Seu model + API key env
    litellm_settings:
      mcp_servers:
        sequential-thinking:
          transport: stdio
          command: npx
          args: ["-y", "@modelcontextprotocol/server-sequential-thinking"]
          disabled: false
        # + outros MCPs aqui
general_settings:
  store_model_in_db: true # UI/DB
